## 1주차 정리

### 목차
[1. 벡터](#벡터)
[2. 행렬](#행렬)
[3. 경사하강법](#경사하강법)
[4. 딥러닝 학습 방법](#딥러닝-학습-방법)
[5. 확률론](#확률론)
[6. 통계학](#통계학)
[7. 베이즈 통계학](#베이즈-통계학)
[8. CNN](#CNN)

### 벡터
* 벡터는 숫자를 원소로 가지는 리스트 또는 배열이다.
* 벡터의 노름(norm)은 원점에서부터의 거리를 말한다.
  * $L_{1}$-노름은 각 성분의 변화량의 절대값을 모두 더한다. - 맨해탄 거리
  * $L_{1}$-노름은 피타고라스 정리를 이용해 거리를 계산한다. - 유클리드 거리

<br />

### 행렬
* 행렬은 벡터를 원소로 가지는 2차원 배열이다.
* 행렬 곱셈(matrix multiplication)은 $i$번째 행벡터와 $j$번째 열벡터 사이의 내적을 성분으로 가지는 행렬을 계산한다.
* 넘파이의 ```np.inner```은 $i$번째 행벡터와 $i$번째 행벡터 사이의 내적을 성분으로 가지는 행렬을 계산한다.
* 벡터공간에서 행렬곱을 통하여 벡터를 다른 차원의 공간으로 보낼 수 있다.
* 역행렬을 계산할 수 없을 때, 유사역행렬(pseudo-inverse) 또는 무어-펜로즈(Moore-Penrose) 역행렬 $A^{+}$ 을 이용한다.
  * ```np.lianlg.pinv(X)```
  * $n \geq m$인 경우 $A^{+} = (A^{T}A)^{-1}A^{T}$
  * $n \leq m$인 경우 $A^{+} = A^{T}(AA^{T})^{-1}$
* ```np.lianlg.pinv``` 를 이용하여 데이터를 선형모델로 해석하는 선형회귀식을 찾을 수 있다. 

<br />

### 경사하강법
* 한 점에서 접선의 기울기를 알면 어느 방향으로 점을 움직여야 함수값이 증가하는지/감소하는지 알 수 있다.
  * 증가시키고 싶다면 미분값을 더한다.
  * 감소시키고 싶으면 미분값을 뺀다.
* 미분값을 빼면 경사하강법(gradient descent)이라 하며, 함수의 극소값의 위치를 구할 때 사용한다.
``` python
# Input: gradient, init, lr, eps
# Output: var

# gradient: 미분을 계산하는 함수
# init: 시작점,  lr: 학습률,  eps: 알고리즘 종료조건

var = init
grad = gradient(var)
while abs(grad) > eps:
  var = var - lr * grad
  grad = gradient(var)
```
* 변수가 벡터일 경우엔 편미분을 사용하여, 그레디언트 벡터를 이용하여 경사하강법에 적용한다.
* 경사하강법 기반 선형회귀 알고리즘
> ``` python
> # Input: X, y, lr, T
> # Output: beta
>
> # norm: L2-노름을 계산하는 함수
> # lr: 학습률,  T: 학습횟수
>
> for t in range(T):
>   error = y - X @ beta
>   grad = - transpose(X) @ error
>   beta = beta - lr * grad
> ```
* 경사하강법은 미분가능하고 볼록(convex)한 함수에 대하여 적절한 학습률과 학습횟수를 선택했을 때 수렴이 보장되어 있다.
* 확률적 경사하강법(Stochastic Gradient Descent)모든 데이터를 사용해서 업데이트하는 대신 데이터 한개 또는 일부 활용하여 업데이트한다.
* 볼록이 아닌(non-convex) 목적식은 SGD를 통하여 최적화할 수 있다.
* SGD는 미니배치 $\mathcal{D}_{(b)}=(X_{(b)},y_{(b)}) \subset \mathcal{D}$ 를 가지고 그레디언트 벡터를 계산한다.
* SGD는 볼록이 아닌 목적식에서도 사용 가능하므로 경사하강법보다 머신러닝 학습에 더 효율적이다.

<br />

### 딥러닝 학습 방법
* 소프트맥스 함수
  * 소프트맥스(softmax) 함수는 모델의 출력을 확률로 해석할 수 있게 변환해주는 연산이다.
  * 분류 문제를 풀 때 선형모델과 소프트맥스 함수를 결합하여 예측한다.
* 신경망
  * 신경망은 선형모델과 활성함수를 합성한 함수이다.
  * 활성함수란 $R$위에 정의된 비선형 함수로서 딥러닝에서 매우 중요한 개념이다.
  * 활성함수를 쓰지 않으면 딥러닝은 선형모형과 차이가 없다.
  * 시그모이드(sigmoid) 함수나 tanh 함수는 전통적으로 많이 쓰이던 활성함수지만, 딥러닝에선 ReLU 함수를 많이 쓰고 있다.
  * 다층(multi-layer) 퍼셉트론(MLP)은 신경망이 여러층 합성된 함수이다.
  * 층을 여러개 쌓는 이유: 층이 깊을수록 목적함수를 근사하는데 필요한 뉴런(노드)의 숫자가 훨씬 빨리 줄어들어 좀 더 효율적으로 학습이 가능하기 때문.
* 역전파
  * 딥러닝은 역전파(backpropagation) 알고리즘을 이용하여 각 층에 사용된 패러미터를 학습한다.
  * 각 층 패러미터의 그레디언트 벡터는 윗층부터 역순으로 계산한다.
  * 역전파 알고리즘은 합성함수 미분법인 연쇄법칙 기반 자동미분을 사용한다.

<br />

### 확률론
* 딥러닝은 확률론 기반의 기계학습 이론에 바탕을 두고있다.
* 회귀 분석에서 손실함수로 사용되는 $L_{2}$-노름은 예측오차의 분산을 가장 최소화하는 방향으로 학습하도록 유도한다.
* 분류 문제에서 사용되는 교차엔트로피(cross-entropy)는 모델 예측의 불확실성을 최소화하는 방향으로 학습하도록 유도한다.
* 확률변수는 확률분포에 따라 이산형(discrete)과 연속형(continuous) 확률변수로 구분한다.
  * 이산형 확률변수는 확률변수가 가질 수 있는 경우의 수를 모두 고려하여 확률을 더해서 모델링한다.
  $$\mathbb{P}(X \in A) = \sum_{x \in A} P(X = x)$$
  * 연속형 확률변수는 데이터 공간에 정의된 확률변수의 밀도(density) 위에서의 적분을 통하여 모델링한다.
  $$\mathbb{P}(X \in A) = \int_{A} P(x)dx$$
* 결합분포 $P(x, y)$는 데이터공간을 모델링한다.
* 조건부확률분포 $P(x|y)$는 데이터 공간에서 입력 x와 출력 y 사이의 관계를 모델링한다.
* 몬테카를로 샘플링
  * 기계학습의 많은 문제들은 확률분포를 명시적으로 모를 때가 대부분인데, 이 때 데이터를 이용하여 기대값을 계산하려면 몬테카를로(Monte Carlo) 샘플링 방볍을 사용해아 한다.
  * 몬테카를로 샘플링은 독립추출만 보장된다면 대수의 법칙에 의해 수렴성을 보장한다.

<br />

### 통계학
* 데이터가 특정 확률분포를 따른다고 선험적으로(a priori) 가정한 후 그 분포를 결정하는 모수(parameter)를 추정하는 방법을 모수적(parametric) 방법론이라 한다.
* 특정 확률분포를 가정하지 않고 데이터에 따라 모델의 구조 및 모수의 개수가 유연하게 바뀌면 비모수(nonparametric) 방법론이라 한다.
* 기계적으로 확률분포를 가정해서는 안되며, 데이터를 생성하는 원리를 먼저 고려해야한다.
* 통계량의 확률분포를 표집분포(sampling distribution)라 부르며, 특히 표본평균의 표집분포는 $N$이 커질수록 정규분포 $N(\mu, \sigma^2/N)$ 를 따른다.
* 최대가능도 추정법
  * 이론적으로 가장 가능성이 높은 모수를 추정하는 방법 중 하나는 최대가능도 추정법(maximum likelihood estimation, MLE)이다.
  $$\hat{\theta}_{MLE} = \underset{\theta}{argmax} L(\theta ; x) = \underset{\theta}{argmax} P(x|\theta)$$
  * 데이터 집합 $X$가 독립적으로 추출되었을 경우 로그가능도를 최적화한다.
    * 로그가능도를 최적화하는 모수 $\theta$는 가능도를 최적화하는 MLErk ehlsek.
    * 로그가능도를 사용하면 연산량을 $O(n^2)$에서 $O(n)$으로 줄여준다.

<br />

### 베이즈 통계학
* 조건부 확률 $P(A|B)$는 사건 $B$가 일어난 상황에서 사건 $A$가 발생할 확률을 의미힌다.
$$P(A \cap B) = P(B)P(A|B)$$
$$P(\theta|\mathfrak{D}) = P(\theta) \; {P(\mathfrak{D} | \theta) \over P(\mathfrak{D})} \qquad \underset{(사후확률)}{posterior} = \underset{(사전확률)}{prior}{\underset{(가능도)}{likelihood} \over Evidence}
$$
* 베이즈 정리를 통해 새로운 데이터가 들어왔을 때 앞서 계산한 사후확률을 사전확률로 사용하여 갱신된 사후확률을 계산할 수 있다.
* 조건부 확률은 유용한 통계적 해석을 제공하지만 인과관계를 추론할 때 함부로 사용해선 안된다.
* 인과관계는 데이터 분포의 변화에 강건한 예측모형을 만들 때 필요하다.
* 인과관계를 알아내기 위해서는 중첩요인(confounding factor)의 효과를 제거하고 원인에 해당하는 변수만의 인과관계를 계산해야 한다.

<br />

### CNN
* Convolution 연산은 커널(kernel)을 입력벡터 상에서 움직여가면서 선형모델과 합성함수가 적용되는 구조이다.
* Convolution 연산의 수학적인 의미는 신호(signal)를 커널을 이용해 국소적으로 증폭 또는 감소시켜서 정보를 추출 또는 필터링하는 것이다.
$$continuous \qquad [f*g](x) = \int_{\mathbb{R}^{d}}f(z)g(x+z)\,dz = \int_{\mathbb{R}^{d}}f(x+z)g(z)\,dz = [g*f](x)$$
$$discrete \qquad [f*g](i) = \sum_{a\in\mathbb{Z}^{d}}f(a)g(i+a) = \sum_{a\in\mathbb{Z}^d}f(i+a)g(a) = [g*f](i)$$
* 커널은 정의역 내에서 움직여도 변하지 않고, 주어진 신호에 국소적(local)을 적용한다.
* 2차원 Convolution 연산에서 입력 크기를 $(H, W)$, 커널 크기를 $(K_H, K_W)$, 출력 크기를 $(O_H, O_W)$라 하면 출력 크기는 다음과 같이 계산한다.
$$O_H = H - K_H + 1$$
$$O_W = W - K_W + 1$$
* Convolution 연산의 역전파를 계산할 때도 convolution 연산이 나오게 된다.